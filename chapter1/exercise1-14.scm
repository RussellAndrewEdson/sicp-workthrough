;;; SICP Exercise 1.14
;;; (The online text can be found at 
;;;     http://mitpress.mit.edu/sicp/full-text/book/book.html)
;;;
;;; This exercise had us draw the tree for the process generated by the
;;; count-change procedure in making change for 11 cents. We then had to 
;;; determine the orders of growth of the space and number of steps used
;;; by the process as the amount to be changed increased.

; The graph was much too big and complicated to look good here in text form,
; so I've drawn it up (admittedly it's rather crude) and scanned it in 
; a separate file (see exercise1-14_tree.pdf).


; Hopefully the drawing over in the other file will kind of aid the following
; explanation of what's going on (so that we can deduce the orders of growth.)

; Consider the general case, where we want to call the count-change
; procedure to work on an amount N to be changed.
; That is, suppose we call (count-change N).
; For simplicity, suppose we have the same 5 kinds of coins as given in the
; procedure specified in the book.

; Then notice (as I've shown along the top of the tree in the drawing) that
; we always have the following 5 calls:
;   (cc N 5)
;   (cc N 4)
;   (cc N 3)
;   (cc N 2)
;   (cc N 1)


; Now a call to (cc N 1) will yield N more calls as N gets decremented to 0:
;   (cc (N-1) 1)
;   (cc (N-2) 1)
;   ...
;   (cc (N-N) 1)

; Each of those calls (except the last one, but including the original
; call to (cc N 1) ) will also yield a call to (cc X 0):
;   (cc N 0)
;   (cc (N-1) 0)
;   (cc (N-2) 0)
;   ...
;   (cc (N-(N-1)) 0)

; So for the call to (cc N 1), we get exactly 2*N recursive calls.

; (Note here that in terms of space, we can see that the (cc N 1) -> (cc 0 1)
; will be the longest chain of delayed operations that we have in the
; count-change procedure, and those calls will have been completely wrapped up
; before we start on the other, smaller branches. So in terms of space used, 
; we only keep track of at most (N+5) calls at a time, so we have a Theta(N) 
; order of growth for the space.)


; Next, consider a call to (cc N 2). On the left-hand side, this yields the
; calls to (cc N 1), which we've already counted. However, it also starts a
; chain of calls on the right-hand side that look like this:
;   (cc (N-1(5)) 2)
;   (cc (N-2(5)) 2)
;   (cc (N-3(5)) 2)
;   ...
;   (cc (N-(N/5)(5)) 2)

; Of which there are (N/5) recursive calls.

; * Note that I'm not being too rigorous here. The final call isn't really
; going to be (cc (N-(N/5)(5)) 2) in most cases when you keep in mind that
; this is an integer division. But the important thing is that we stop 
; when that first parameter to cc is negative, which will +/- 1 call 
; of this number, so it'll work for our purposes.

; Further, each of those calls will have their own chains of recursive calls
;   (cc (N-K(5)) 1)
;   (cc (N-K(5)-1) 1)
;   (cc (N-K(5)-2) 1)
;   ...
;   (cc (N-K(5)-(N-K(5)) 1)

; As we saw before, the calls to (cc (N-K(5)) 1) will give us 2*(N-K(5)) calls.
; And this happens for each of the (N/5) recursive calls, so we have 
; [Sum from K=1 to K=N/5 of ( 2*(N-K(5)) )] recursive calls on this branch.

; (And since K=0 corresponds to that first branch of 2*N calls that we counted
; earlier, we have [Sum from K=0 to K=N/5 of ( 2*(N-K(5)) )] total calls so
; far.)


; Next, we have the branch from the call to (cc N 3).
; Similarly, this yields all of the calls to (cc N 2) on its left-hand side,
; and for the right-hand side contains the chain:
;   (cc (N-1(10)) 3)
;   (cc (N-2(10)) 3)
;   ...
;   (cc (N-(N/10)(10)) 3)

; So (keeping in mind the point about the integer division I made earlier),
; we have (N/10) calls here.

; Then each of those calls will have their own chain of calls. If we have the
; call (cc (N-K(10)) 3), then this fires off a call to (cc (N-K(10)) 2), which
; as we showed before has the chain of calls given by the summation where we
; take off 5 each time.


; So now we can sort of see what's going on. Similarly, the call to (cc N 4)
; will yield all of the calls to (cc N 3) on its left-hand side, with the
; following chain on its right-hand side:
;   (cc (N-1(25)) 4)
;   (cc (N-2(25)) 4)
;   ...
;   (cc (N-(N/25)(25)) 4)
;
; Of which there are (N/25) calls, each of which fires off a call to
; (cc (N-K(25)) 3).

; Finally, the same thing happens with the call to (cc N 5) that starts
; everything off. The left-hand side contains the call chain for (cc N 4),
; and the right-hand side contains the chain:
;   (cc (N-1(50)) 5)
;   (cc (N-2(50)) 5)
;   ...
;   (cc (N-(N/50)(50)) 5)
;
; Of which there are (N/50) calls, each of which fires off its own call
; to (cc (N-K(50)) 4).


; So as to not get bogged down in all the math, let's simplify things by
; using the Theta notation at each point. We said before that the call 
; chain for (cc N 1) contained 2*N recursive calls; let's say that it has
; Theta(N) steps. 
;
; Then the number of steps for (cc N 2) becomes the sum from 0 to N/5 
; of Theta(N), or (N/5)*Theta(N). Again, let's simplify this to Theta(N^2).
; (As N/5 is Theta(N), being a constant multiple of N.)
;
; Then the number of steps for (cc N 3) is similarly the sum from 0 to N/10
; of Theta(N^2), which becomes Theta(N^3).
;
; Similarly, the number of steps for (cc N 4) is Theta(N^4),
; and finally the number of steps for (cc N 5), and hence the count-change
; procedure itself, is Theta(N^5).


; So the orders of growth in terms of computational resources for the
; count-change procedure are as follows.
;
; Let N denote the amount to make change for. Then we have:
;     order of growth for number of steps: Theta(N^5)
;     order of growth for space: Theta(N).